{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6734765f-cd55-41ca-9db3-fe12dfb53406",
   "metadata": {},
   "source": [
    "# MLP sign MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f407f93-802b-4b03-983d-c1c0766e3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Imports\n",
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "%pip install kaggle\n",
    "%pip install distutils\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d9ca3-df2f-48c0-918f-8645431cd17d",
   "metadata": {},
   "source": [
    "## Task 1: Acquire the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1a3f8c-365d-4590-b907-d1f785c90f7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_train/sign_mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUsers/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_train/sign_mnist_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsers/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_test/sign_mnist_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_train/sign_mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv('Users/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_train/sign_mnist_train.csv')\n",
    "test_df=pd.read_csv('Users/mayaarvanitis/Desktop/winter2024/COMP551/archive/sign_mnist_test/sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dfcab-3af3-4a35-be68-5a9299bc5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddac127-55c7-41d2-87b3-0cc464b2c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d6345-ede9-4f89-b6d2-37e91c24e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4ba70-d0b5-4775-9617-7548e392eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 0,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  shear_range=0,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "#Normalize Features\n",
    "#Drop the labels\n",
    "X_train = train_df.drop('label', axis=1).values / 255.0\n",
    "X_test = test_df.drop('label', axis=1).values / 255.0\n",
    "y_train = LabelBinarizer().fit_transform(train_df['label'])\n",
    "y_test = LabelBinarizer().fit_transform(test_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dd072-c849-4019-8108-64ba3878b7cd",
   "metadata": {},
   "source": [
    "#### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2c78f-f70f-4882-b8ec-cd84d0583595",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axe=plt.subplots(2,2)\n",
    "fig.suptitle('Preview of dataset')\n",
    "axe[0,0].imshow(X_train[0].reshape(28,28),cmap='gray')\n",
    "axe[0,0].set_title('label: 3  letter: C')\n",
    "axe[0,1].imshow(X_train[1].reshape(28,28),cmap='gray')\n",
    "axe[0,1].set_title('label: 6  letter: F')\n",
    "axe[1,0].imshow(X_train[2].reshape(28,28),cmap='gray')\n",
    "axe[1,0].set_title('label: 2  letter: B')\n",
    "axe[1,1].imshow(X_train[4].reshape(28,28),cmap='gray')\n",
    "axe[1,1].set_title('label: 13  letter: M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad1fcd-6414-44df-960e-ac346a893208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \" + str(X_train.shape))\n",
    "print(\"Shape of y_train: \" + str(y_train.shape))\n",
    "print(\"Shape of X_test: \" + str(X_test.shape))\n",
    "print(\"Shape of y_test: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e53fb-0adc-442b-883c-0f523f10abcb",
   "metadata": {},
   "source": [
    "## Task 2: Implement an MLP to classify image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd33fa4-5274-4fa3-a7bf-08363c6660f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for all activation functions\n",
    "class ActivationFunction:\n",
    "    # Static method for the ReLU (Rectified Linear Unit) activation function.\n",
    "    # ReLU is defined as the positive part of its input.\n",
    "    # Input: A numpy array 'x'.\n",
    "    # Output: A numpy array where each element is the max between 0 and the corresponding element in 'x'.\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Static method for the derivative of the ReLU function.\n",
    "    # This is used in backpropagation during neural network training.\n",
    "    # Input: A numpy array 'x'.\n",
    "    # Output: A numpy array where each element is 1 if the corresponding element in 'x' is greater than 0, otherwise 0.\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    # Static method for the softmax function.\n",
    "    # Softmax is often used in the output layer of a neural network for multi-class classification.\n",
    "    # It converts the input array into a probability distribution.\n",
    "    # Input: A 2D numpy array 'x'. Each row represents a set of logits for a single sample.\n",
    "    # Output: A 2D numpy array where each row sums to 1 and represents the probability distribution for the corresponding sample.\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # subtract max for numerical stability\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    # Static method for the sigmoid function.\n",
    "    # Sigmoid is often used in the output layer of a binary classification neural network.\n",
    "    # It converts the input into a value between 0 and 1.\n",
    "    # Input: A numpy array 'x'.\n",
    "    # Output: A numpy array where each element is the sigmoid of the corresponding element in 'x'.\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "      clipped_x = np.clip(x, -500, 500)  # clip input to prevent overflow in the exponential function\n",
    "      return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "    # Static method for the Leaky ReLU activation function.\n",
    "    # Leaky ReLU allows a small, non-zero gradient when the input is negative.\n",
    "    # Input: A numpy array 'x' and an optional slope parameter 'alpha' for negative inputs.\n",
    "    # Output: A numpy array where each element is either the corresponding element in 'x' (if positive) or 'alpha' times that element (if negative).\n",
    "    @staticmethod\n",
    "    def leaky_relu(x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad3a08-5a73-4237-a8f5-1a1e9d467689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definition for a Multi-Layer Perceptron (MLP) neural network.\n",
    "class MLP:\n",
    "\n",
    "    # Constructor for the MLP class.\n",
    "    # Inputs:\n",
    "    # - layer_sizes: A list of integers representing the sizes of each layer in the network.\n",
    "    # - learning_rate: A float representing the learning rate for gradient descent.\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights with random values scaled by the square root of 2 divided by the size of the previous layer.\n",
    "        self.weights = [np.random.randn(prev_layer, next_layer) * np.sqrt(2. / prev_layer)\n",
    "                        for prev_layer, next_layer in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        # Initialize biases as zero vectors.\n",
    "        self.biases = [np.zeros((1, next_layer)) for next_layer in layer_sizes[1:]]\n",
    "\n",
    "    # Method for forward propagation.\n",
    "    # Inputs:\n",
    "    # - X: A numpy array representing the input data.\n",
    "    # - activation: A string specifying the activation function to use ('relu', 'sigmoid', or 'leakyrelu').\n",
    "    # Outputs:\n",
    "    # - activations: A list of numpy arrays representing the activations of each layer.\n",
    "    # - zs: A list of numpy arrays representing the weighted sums before activation in each layer.\n",
    "    def forward(self, X, activation='relu'):\n",
    "        self.activations = [X]\n",
    "        self.zs = []\n",
    "\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(self.activations[-1], w) + b\n",
    "            self.zs.append(z)\n",
    "            if activation == 'relu':\n",
    "                activation_output = ActivationFunction.relu(z)\n",
    "            elif activation == 'sigmoid':\n",
    "                activation_output = ActivationFunction.sigmoid(z)\n",
    "            elif activation == 'leakyrelu':\n",
    "                activation_output = ActivationFunction.leaky_relu(z)\n",
    "            else:\n",
    "                raise ValueError(\"Activation function not supported.\")\n",
    "            self.activations.append(activation_output)\n",
    "\n",
    "        # Apply softmax activation to the output layer.\n",
    "        self.activations[-1] = ActivationFunction.softmax(self.zs[-1])\n",
    "        return self.activations, self.zs\n",
    "\n",
    "    # Method for backward propagation.\n",
    "    # Inputs:\n",
    "    # - X: A numpy array representing the input data.\n",
    "    # - y: A numpy array representing the target labels.\n",
    "    # - activations: A list of numpy arrays representing the activations of each layer.\n",
    "    # - zs: A list of numpy arrays representing the weighted sums before activation in each layer.\n",
    "    # Outputs:\n",
    "    # - dW: A list of numpy arrays representing the gradients of the loss with respect to the weights.\n",
    "    # - dB: A list of numpy arrays representing the gradients of the loss with respect to the biases.\n",
    "    def backward(self, X, y, activations, zs):\n",
    "      # Calculate the gradient of the loss with respect to the output layer's activations.\n",
    "      delta = activations[-1] - y\n",
    "      # Compute the gradient of the loss with respect to the last layer's weights.\n",
    "      dW = [np.dot(activations[-2].T, delta)]\n",
    "      # Compute the gradient of the loss with respect to the last layer's biases.\n",
    "      dB = [np.sum(delta, axis=0)]\n",
    "\n",
    "      # Iterate backwards through the layers to propagate the gradients.\n",
    "      for l in range(2, len(self.weights) + 1):\n",
    "          # Get the weighted sum before activation for the current layer.\n",
    "          z = zs[-l]\n",
    "          # Compute the derivative of the ReLU activation function for the current layer.\n",
    "          sp = ActivationFunction.relu_derivative(z)\n",
    "          # Update the delta by propagating it backwards and applying the derivative of the activation function.\n",
    "          delta = np.dot(delta, self.weights[-l+1].T) * sp\n",
    "          # Compute the gradient of the loss with respect to the current layer's weights.\n",
    "          dW.insert(0, np.dot(activations[-l-1].T, delta))\n",
    "          # Compute the gradient of the loss with respect to the current layer's biases.\n",
    "          dB.insert(0, np.sum(delta, axis=0))\n",
    "\n",
    "      return dW, dB\n",
    "\n",
    "    # Method to update the weights and biases based on the gradients.\n",
    "    # Inputs:\n",
    "    # - dW: A list of numpy arrays representing the gradients of the loss with respect to the weights.\n",
    "    # - dB: A list of numpy arrays representing the gradients of the loss with respect to the biases.\n",
    "    def update_params(self, dW, dB):\n",
    "        self.weights = [w - self.learning_rate * dw for w, dw in zip(self.weights, dW)]\n",
    "        self.biases = [b - self.learning_rate * db for b, db in zip(self.biases, dB)]\n",
    "\n",
    "    # Method to compute the loss using the cross-entropy function.\n",
    "    # Inputs:\n",
    "    # - y_true: A numpy array representing the true labels.\n",
    "    # - y_pred: A numpy array representing the predicted probabilities.\n",
    "    # Output: A float representing the loss value.\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "      m = y_true.shape[0]\n",
    "      log_probs = np.log(np.clip(y_pred, a_min=1e-15, a_max=1))\n",
    "      loss = -np.sum(y_true * log_probs) / m\n",
    "      return loss\n",
    "\n",
    "    # Method to predict the class labels for the input data.\n",
    "    # Input: A numpy array representing the input data 'X'.\n",
    "    # Output: A numpy array representing the predicted class labels.\n",
    "    def predict(self, X):\n",
    "        # Perform forward propagation to compute the activations of the network.\n",
    "        activations, _ = self.forward(X)\n",
    "        # Return the indices of the maximum values in the output layer (softmax probabilities),\n",
    "        # which correspond to the predicted class labels.\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "\n",
    "    # Method to train the MLP model.\n",
    "    # Inputs:\n",
    "    # - X: A numpy array representing the input data.\n",
    "    # - y: A numpy array representing the target labels.\n",
    "    # - epochs: An integer representing the number of iterations to train the model.\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            # Perform forward propagation to compute the activations and weighted sums.\n",
    "            activations, zs = self.forward(X)\n",
    "            # Compute the loss using the cross-entropy function.\n",
    "            loss = self.compute_loss(y, activations[-1])\n",
    "            # Perform backward propagation to compute the gradients of the loss with respect to weights and biases.\n",
    "            dW, dB = self.backward(X, y, activations, zs)\n",
    "            # Update the weights and biases using the gradients.\n",
    "            self.update_params(dW, dB)\n",
    "\n",
    "            # Print the loss every 10 epochs.\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # Method to evaluate the accuracy of the MLP model.\n",
    "    # Inputs:\n",
    "    # - X: A numpy array representing the input data.\n",
    "    # - y: A numpy array representing the true labels.\n",
    "    # Output: A float representing the accuracy of the model.\n",
    "    def evaluate_acc(self, X, y):\n",
    "        # Use the predict method to get the predicted class labels.\n",
    "        predictions = self.predict(X)\n",
    "        # Calculate the accuracy by comparing the predicted labels with the true labels.\n",
    "        accuracy = sum(predictions == y.argmax(axis=1)) / y.shape[0]\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdeeca-abce-435c-a4b0-ca6758b1069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# one hot encode y\n",
    "encoder = OneHotEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(\"Before encoding : \")\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(\"After encoding : \")\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef658dd-50d2-4e81-abd5-839764180bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of classes\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "input_size = 784 # we have 784 pixels (Features)\n",
    "output_classes = 24 # we have 24 labels\n",
    "\n",
    "mlp_model = MLP(layer_sizes=[input_size, 128, 64, output_classes], learning_rate=0.01) #setting learning rate to 0.01 to begin\n",
    "\n",
    "#Printing shapes for debugging purposes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train_encoded.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train_encoded = label_binarizer.fit_transform(y_train)\n",
    "y_test_encoded = label_binarizer.transform(y_test)\n",
    "\n",
    "\n",
    "# Training Model ================================\n",
    "mlp_model.fit(X_train, y_train_encoded, epochs=100)\n",
    "\n",
    "# Evaluate the Model ================================\n",
    "train_accuracy = mlp_model.evaluate_acc(X_train, y_train_encoded)\n",
    "test_accuracy = mlp_model.evaluate_acc(X_test, y_test_encoded)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd67fb-4c14-4e6e-b476-17ef2d9f9dd4",
   "metadata": {},
   "source": [
    "## Task 3: Running experiments\n",
    "### Experiment 1: Three Different MLP Models\n",
    "Three different models: (1) an MLP with no hidden layer, (2) an MLP with a single hidden layer having ReLU activations, (3) an MLP with 2 hidden layers having ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f8047-f305-415a-bd4f-d64e1f402f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_mlps = [\n",
    "    ([784, 24], 'No Hidden Layer'),\n",
    "    ([784, 64, 24], 'Single Hidden Layer'),\n",
    "    ([784, 128, 128, 24], 'Two Hidden Layers')\n",
    "]\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for model, name in three_mlps:\n",
    "    mlp = MLP(model, learning_rate = 0.001)\n",
    "    #train the model\n",
    "    mlp.fit(X_train, y_train_encoded, epochs=10)\n",
    "\n",
    "    #evaluate on training set\n",
    "    train_accuracy = mlp.evaluate_acc(X_train, y_train_encoded)\n",
    "\n",
    "    #evaluate on test set\n",
    "    test_accuracy = mlp.evaluate_acc(X_test, y_test_encoded)\n",
    "\n",
    "    accuracies.append((name, train_accuracy, test_accuracy))\n",
    "\n",
    "#print accuracies for each model\n",
    "for name, train_accuracy, test_accuracy in accuracies:\n",
    "    print(f\"Model: {name}, Train Accuracy: {train_accuracy*100:.2f}%, Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebe347-5f49-4490-a0bd-d0634d4212d6",
   "metadata": {},
   "source": [
    "Update the MLP class accordingly & then apply L2 regularization to the MLP with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740755f-5ae3-4ac3-9244-ab80b7c47390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, reg_lambda=0.01): # add reg_lambda parameter for L2 regularization\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda # store regularization parameter\n",
    "        self.weights = [np.random.randn(prev_layer, next_layer) * np.sqrt(2. / prev_layer)\n",
    "                        for prev_layer, next_layer in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        self.biases = [np.zeros((1, next_layer)) for next_layer in layer_sizes[1:]]\n",
    "\n",
    "    def forward(self, X, activation='relu'):\n",
    "        self.activations = [X]\n",
    "        self.zs = []\n",
    "\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(self.activations[-1], w) + b\n",
    "            self.zs.append(z)\n",
    "            if activation == 'relu':\n",
    "                activation_output = ActivationFunction.relu(z)\n",
    "            elif activation == 'sigmoid':\n",
    "                activation_output = ActivationFunction.sigmoid(z)\n",
    "            elif activation == 'leakyrelu':\n",
    "                activation_output = ActivationFunction.leaky_relu(z)\n",
    "            else:\n",
    "                raise ValueError(\"Activation function not supported.\")\n",
    "            self.activations.append(activation_output)\n",
    "\n",
    "        self.activations[-1] = ActivationFunction.softmax(self.zs[-1])\n",
    "        return self.activations, self.zs\n",
    "\n",
    "    def backward(self, X, y, activations, zs):\n",
    "        delta = activations[-1] - y\n",
    "        dW = [np.dot(activations[-2].T, delta)]\n",
    "        dB = [np.sum(delta, axis=0)]\n",
    "\n",
    "        for l in range(2, len(self.weights) + 1):\n",
    "            z = zs[-l]\n",
    "            sp = ActivationFunction.relu_derivative(z)\n",
    "            delta = np.dot(delta, self.weights[-l+1].T) * sp\n",
    "            dW.insert(0, np.dot(activations[-l-1].T, delta) + self.reg_lambda * self.weights[-l])  # Add L2 regularization term\n",
    "            dB.insert(0, np.sum(delta, axis=0))\n",
    "\n",
    "        return dW, dB\n",
    "\n",
    "    def update_params(self, dW, dB):\n",
    "        self.weights = [w - self.learning_rate * dw for w, dw in zip(self.weights, dW)]\n",
    "        self.biases = [b - self.learning_rate * db for b, db in zip(self.biases, dB)]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        log_probs = np.log(np.clip(y_pred, a_min=1e-15, a_max=1))\n",
    "        loss = -np.sum(y_true * log_probs) / m\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            activations, zs = self.forward(X)\n",
    "            loss = self.compute_loss(y, activations[-1])\n",
    "            dW, dB = self.backward(X, y, activations, zs)\n",
    "            self.update_params(dW, dB)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def evaluate_acc(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = sum(predictions == y.argmax(axis=1)) / y.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ec22d-45bb-405f-a32b-841bd855cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model architecture and create the new MLP object\n",
    "mlp_regularized = MLP(layer_sizes=[784, 128, 128, 24], learning_rate=0.01, reg_lambda=0.0001)\n",
    "\n",
    "#train the MLP with L2 regularization\n",
    "mlp_regularized.fit(X_train, y_train_encoded, epochs=300)\n",
    "\n",
    "#evaluate the performance of the trained model on the test data\n",
    "accuracy_L2reg = mlp_regularized.evaluate_acc(X_test, y_test_encoded)\n",
    "print(\"Test Accuracy - Regularized Model with L2 regularization: {:.2f}%\".format(accuracy_L2reg * 100))\n",
    "\n",
    "#evaluate training accuracy\n",
    "accuracy_L2reg_train = mlp_regularized.evaluate_acc(X_train, y_train_encoded)\n",
    "print(\"Train Accuracy - Regularized Model with L2 regularization: {:.2f}%\".format(accuracy_L2reg_train * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13936ae2-afe4-42bd-98c0-cbfa6afff4b1",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083d172-ec85-4da4-aefa-aea735c48bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing and reshaping features\n",
    "X_train = train_df.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = test_df.drop('label', axis=1).values.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# one-hot encoding labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(train_df['label'])\n",
    "y_test = label_binarizer.transform(test_df['label'])\n",
    "\n",
    "# defining image data generator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0,\n",
    "    height_shift_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    shear_range=0,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# defining the ConvNet architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(24, activation='softmax')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
